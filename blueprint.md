# ğŸ”Œ AI/ML Daily Digest â€” Complete MCP Implementation Blueprint
### Zero Scrapers Â· Zero Cost Â· 100% Automated Â· Works Even When Your PC is OFF

---

## ğŸ“‹ Table of Contents

1. [Critical First â€” What Happens When Your PC is Off?](#1-critical-first--what-happens-when-your-pc-is-off)
2. [What is MCP and Why Use it Instead of Scrapers?](#2-what-is-mcp-and-why-use-it-instead-of-scrapers)
3. [The Big Idea â€” 3 Servers Instead of 13](#3-the-big-idea--3-servers-instead-of-13)
4. [Full System Architecture](#4-full-system-architecture)
5. [What is FastMCP and How Do You Build With It?](#5-what-is-fastmcp-and-how-do-you-build-with-it)
6. [Server 1 â€” Research Server (Papers, RSS, Blogs)](#6-server-1--research-server-papers-rss-blogs)
7. [Server 2 â€” Community Server (Reddit, HN, HuggingFace)](#7-server-2--community-server-reddit-hn-huggingface)
8. [Server 3 â€” Utility Server (Crawl, Memory, Search)](#8-server-3--utility-server-crawl-memory-search)
9. [Already Cloud-Hosted MCPs â€” Zero Setup Needed](#9-already-cloud-hosted-mcps--zero-setup-needed)
10. [Complete Master Source Coverage Map](#10-complete-master-source-coverage-map)
11. [How to Deploy to Prefect Horizon (Step by Step)](#11-how-to-deploy-to-prefect-horizon-step-by-step)
12. [GitHub Actions â€” The Free 24/7 Cron Trigger](#12-github-actions--the-free-247-cron-trigger)
13. [Complete Pipeline Flow (PC Off, Everything Automated)](#13-complete-pipeline-flow-pc-off-everything-automated)
14. [Project Folder Structure â€” All 3 Repos](#14-project-folder-structure--all-3-repos)
15. [How Each Tool Works â€” Source by Source](#15-how-each-tool-works--source-by-source)
16. [Detailed Sample Outputs](#16-detailed-sample-outputs)
17. [Step-by-Step Build Timeline â€” Scratch to Production](#17-step-by-step-build-timeline--scratch-to-production)
18. [Complete Accounts & Keys Setup](#18-complete-accounts--keys-setup)
19. [Final Cost Summary](#19-final-cost-summary)
20. [Final Launch Checklist](#20-final-launch-checklist)

---

## 1. Critical First â€” What Happens When Your PC is Off?

> This is the most important question. Everything else builds on the answer.

### The Problem With Self-Hosted MCP Servers

When people talk about "self-hosted MCP servers," they mean the server process runs on your own machine. If your machine is off, the server is off. GitHub Actions is a cloud service â€” it runs on GitHub's servers â€” but if it tries to call an MCP server that lives on your PC and your PC is off, the call fails and you get nothing.

### The Solution â€” Cloud-Deploy Everything

The answer is to deploy all your MCP servers to **Prefect Horizon**, a free cloud platform built specifically for MCP servers. Your servers run on Horizon's infrastructure 24 hours a day, 7 days a week, regardless of whether your PC is on, off, asleep, or broken.

```
WRONG SETUP (PC must be always on):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
GitHub Actions (cloud)
        â†“  calls
Your PC (must be ON and running MCP servers)
        â†“  if PC is off
        âœ—  Pipeline fails. No digest. No notifications.


CORRECT SETUP (PC never needed):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
GitHub Actions (cloud)
        â†“  calls
Prefect Horizon URLs (cloud, always online 24/7)
        â†“  returns data
Gemini Flash 2.0 (cloud)
        â†“  saves
Supabase (cloud)
        â†“  sends to
Email + Telegram + Discord
        â†“
Dashboard on Vercel (cloud)

YOUR PC = NEVER NEEDED âœ…
```

### The Three Layers That Make This Work

```
LAYER 1 â€” Trigger (GitHub Actions)
  Runs on GitHub's cloud servers
  Wakes up at 7:00 AM IST every day
  Your PC does not need to be on
  Free: 2,000 minutes/month

LAYER 2 â€” MCP Servers (Prefect Horizon)
  Your custom FastMCP servers deployed to Horizon
  Run 24/7 on Horizon's cloud infrastructure
  Free for personal projects
  Auto-redeploy on every git push

LAYER 3 â€” External Cloud MCPs (GitHub, HF, Supabase)
  Already hosted by their own companies
  No deployment needed at all
  Just use their URLs
  Free with free accounts
```

---

## 2. What is MCP and Why Use it Instead of Scrapers?

### MCP (Model Context Protocol) â€” Simple Explanation

MCP is an open standard created by Anthropic in late 2024. Think of it as a universal connector â€” like a USB port â€” between AI systems and data sources. Instead of writing custom scraping code for every website, you plug in a pre-built MCP server that already handles all the API calls, authentication, parsing, and error handling for that source.

```
SCRAPER APPROACH (what you wanted to avoid):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
You write Python scraper for ArXiv
  â†’ ArXiv changes their HTML structure
  â†’ Your scraper breaks
  â†’ You spend 2 hours debugging
  â†’ Fix it
  â†’ It breaks again next month

You write Python scraper for Reddit
  â†’ Reddit changes their API rules
  â†’ Your scraper breaks
  â†’ Reddit starts blocking requests
  â†’ You debug again

Repeat this for every single source. Forever.
Maintenance nightmare.


MCP APPROACH (what this blueprint uses):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
arxiv-mcp-server â†’ already handles everything for ArXiv
reddit-mcp-buddy  â†’ already handles everything for Reddit
github-mcp-server â†’ already handles everything for GitHub

You just CALL the tool with parameters.
Community maintains the server.
When APIs change, someone else fixes it.
You focus on your pipeline logic, not scraping code.
```

### How MCP Communication Works

```
YOUR PIPELINE                    MCP SERVER               DATA SOURCE
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€       â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
"Give me latest papers           arxiv-mcp-server         ArXiv API
 from cs.AI last 24h"      â†’     receives request    â†’    queries API
                                  formats response         returns JSON
                           â†     returns items       â†
  [{title, authors,
    abstract, url,
    published_at}]

The pipeline never touches ArXiv directly.
The MCP server is the intermediary.
```

### FastMCP â€” The Framework for Building MCP Servers

FastMCP is the standard framework for writing MCP servers in Python. It was created shortly after Anthropic announced MCP, was incorporated into the official MCP Python SDK, and today powers 70% of all MCP servers. It is downloaded over one million times a day.

The core concept is simple: you write a regular Python function that fetches data, add a decorator on top of it, and FastMCP automatically turns it into a fully compliant MCP tool. All the protocol complexity â€” schema generation, parameter validation, error handling, connection management â€” is handled for you automatically. You focus purely on the data-fetching logic.

```
THE PATTERN:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. Create a FastMCP server instance with a name
2. Write Python functions that fetch data from APIs
3. Add @mcp.tool decorator above each function
4. FastMCP auto-generates MCP schema from function signature
5. Push to GitHub â†’ deploy to Horizon â†’ get a live URL

That URL is what GitHub Actions calls every morning.
```

---

## 3. The Big Idea â€” 3 Servers Instead of 13

### Your Original Instinct (Correct!)

You correctly identified that creating one GitHub repo per MCP server is wasteful. If you had 13 tools, that's 13 repos, 13 deployments, 13 URLs to manage. Instead, you group all logically-related tools into one FastMCP server. One server = one repo = one Horizon deployment = one URL.

### The 3-Server Grouping Strategy

```
ALL YOUR TOOLS GROUPED INTO 3 LOGICAL SERVERS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

SERVER 1: ai-digest-research-server
  GitHub Repo:   github.com/YOUR_USERNAME/ai-digest-research-server
  Horizon URL:   https://ai-digest-research.YOUR_NAME.fastmcp.app/mcp
  Purpose:       Everything about finding content â€” research papers,
                 blog posts, RSS feeds, any URL
  Tools inside:  arxiv, papers_with_code, semantic_scholar,
                 openreview (conferences), kaggle, rss_feeds,
                 fetch_any_url
  Covers:        Sections 1, 2, 7, 8, 9, 10 of master list


SERVER 2: ai-digest-community-server
  GitHub Repo:   github.com/YOUR_USERNAME/ai-digest-community-server
  Horizon URL:   https://ai-digest-community.YOUR_NAME.fastmcp.app/mcp
  Purpose:       Everything about community signals â€” social platforms,
                 discussions, trending models and tools
  Tools inside:  reddit (5 subreddits), hacker_news,
                 huggingface (models, spaces, papers, leaderboard)
  Covers:        Sections 3, 5 of master list


SERVER 3: ai-digest-utility-server
  GitHub Repo:   github.com/YOUR_USERNAME/ai-digest-utility-server
  Horizon URL:   https://ai-digest-utility.YOUR_NAME.fastmcp.app/mcp
  Purpose:       Supporting tools â€” JS page rendering, memory,
                 search, pipeline utilities
  Tools inside:  crawl4ai (JS pages), searxng (web search),
                 memory (persistent preferences)
  Covers:        Hard-to-reach pages, bot memory, live search


ALREADY CLOUD-HOSTED (zero repos, zero deployment needed):
  GitHub MCP     â†’ api.githubcopilot.com/mcp/          (GitHub hosts it)
  Supabase MCP   â†’ mcp.supabase.com                    (Supabase hosts it)
  Context7 MCP   â†’ via npx command                     (Context7 hosts it)
  HuggingFace MCPâ†’ huggingface.co/mcp                  (HF hosts it)
  Covers:        Section 3 (GitHub repos + releases), Sections 4, 9
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
RESULT:
  3 GitHub repos
  3 Horizon deployments
  4 external cloud MCPs (no setup needed)
  Total: 7 MCP URLs in your config
  Monthly cost: $0
```

---

## 4. Full System Architecture

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘             GITHUB ACTIONS â€” FREE CLOUD CRON SCHEDULER               â•‘
â•‘        Triggers at 7:00 AM IST (1:30 AM UTC) every single day        â•‘
â•‘        Runs on GitHub's servers â€” your PC never needs to be on        â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¦â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                                â•‘
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â–¼                     â–¼                       â–¼
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—  â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—  â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ SERVER 1          â•‘  â•‘ SERVER 2            â•‘  â•‘ CLOUD MCPs            â•‘
â•‘ Research Server   â•‘  â•‘ Community Server    â•‘  â•‘ (already hosted)      â•‘
â•‘ (Prefect Horizon) â•‘  â•‘ (Prefect Horizon)   â•‘  â•‘                       â•‘
â•‘                   â•‘  â•‘                     â•‘  â•‘ GitHub MCP            â•‘
â•‘ Tools:            â•‘  â•‘ Tools:              â•‘  â•‘ â†’ Repos + Releases    â•‘
â•‘ â†’ arxiv           â•‘  â•‘ â†’ reddit            â•‘  â•‘ â†’ Trending repos      â•‘
â•‘ â†’ papers_with_codeâ•‘  â•‘ â†’ hacker_news       â•‘  â•‘ â†’ All 22 framework    â•‘
â•‘ â†’ semantic_scholarâ•‘  â•‘ â†’ huggingface       â•‘  â•‘   releases monitored  â•‘
â•‘ â†’ openreview      â•‘  â•‘   (models, spaces,  â•‘  â•‘                       â•‘
â•‘ â†’ kaggle          â•‘  â•‘    papers,          â•‘  â•‘ Supabase MCP          â•‘
â•‘ â†’ rss_feeds       â•‘  â•‘    leaderboard)     â•‘  â•‘ â†’ Save all results    â•‘
â•‘ â†’ fetch_url       â•‘  â•‘                     â•‘  â•‘ â†’ Query digest data   â•‘
â•‘                   â•‘  â•‘                     â•‘  â•‘                       â•‘
â•‘ OUTPUT:           â•‘  â•‘ OUTPUT:             â•‘  â•‘ Context7 MCP          â•‘
â•‘ ~200 items        â•‘  â•‘ ~150 items          â•‘  â•‘ â†’ Live framework docs  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•  â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
          â”‚                     â”‚                       â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â•‘
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   SERVER 3             â”‚
                    â”‚   Utility Server       â”‚
                    â”‚   (Prefect Horizon)    â”‚
                    â”‚                        â”‚
                    â”‚   Tools:               â”‚
                    â”‚   â†’ crawl4ai           â”‚
                    â”‚   â†’ searxng (search)   â”‚
                    â”‚   â†’ memory             â”‚
                    â”‚                        â”‚
                    â”‚   Used for:            â”‚
                    â”‚   JS-heavy pages,      â”‚
                    â”‚   bot /search command  â”‚
                    â”‚   preference memory    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â•‘
                                â–¼
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                DEDUPLICATION (runs in GitHub Actions)                 â•‘
â•‘          Raw input: ~450-500 items from all servers                   â•‘
â•‘          Stage 1: URL hash check (remove exact duplicates)            â•‘
â•‘          Stage 2: 85% fuzzy title match (remove near-duplicates)      â•‘
â•‘          Output: ~150-250 unique items                                 â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¦â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                                â•‘
                                â–¼
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘              GEMINI FLASH 2.0 â€” 1 SINGLE BATCH API CALL              â•‘
â•‘         All 150-250 items sent at once (1M token context window)      â•‘
â•‘         Returns for each item:                                         â•‘
â•‘           â†’ 2-sentence technical summary                               â•‘
â•‘           â†’ Relevance score 1-10                                       â•‘
â•‘           â†’ is_breaking flag (true/false)                              â•‘
â•‘           â†’ Tags array (LLM, RAG, Agents, MCP, etc.)                  â•‘
â•‘           â†’ Framework mentions array                                   â•‘
â•‘         Free tier: 1,500 requests/day â€” using only 1-2 per day       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¦â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                                â•‘
                                â–¼
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘              SUPABASE (via Supabase MCP â€” cloud hosted)               â•‘
â•‘         Tables: news_items | papers | github_repos | digest_runs      â•‘
â•‘         Free tier: 500MB, 50,000 rows                                  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¦â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                                â•‘
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â–¼              â–¼              â–¼
          ğŸ“§ Resend       ğŸ¤– Telegram     ğŸ® Discord
          Email           Bot Push        Webhook
          Digest          + Commands      Embeds
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â–¼
                    ğŸ–¥ï¸ Next.js Dashboard
                        (Vercel â€” FREE)
                    Always live, reads Supabase directly
```

---

## 5. What is FastMCP and How Do You Build With It?

### Getting FastMCP

FastMCP is a Python library. You get it via pip, the Python package installer. It is hosted on PyPI (the Python Package Index) at `pypi.org/project/fastmcp`. The GitHub repository is at `github.com/PrefectHQ/fastmcp` and the full documentation is at `gofastmcp.com`.

```
WHERE TO GET IT:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Official Documentation:  gofastmcp.com
GitHub Repository:       github.com/PrefectHQ/fastmcp
PyPI Package:            pip install fastmcp
Discord Community:       discord.gg/fastmcp (for help)
```

### The Three Building Blocks of a FastMCP Server

Every FastMCP server is made of three types of components:

```
1. TOOLS
   â†’ Python functions that the pipeline CALLS to fetch data
   â†’ Decorated with @mcp.tool
   â†’ Accepts parameters, returns data
   â†’ Example: fetch_arxiv_papers(category="cs.AI", days=1)
   â†’ This is what you use for 95% of your digest sources

2. RESOURCES
   â†’ Read-only data that can be exposed as reference material
   â†’ Decorated with @mcp.resource
   â†’ Example: expose your rss_sources.yaml as a readable resource
   â†’ Less important for your project

3. PROMPTS
   â†’ Reusable instruction templates
   â†’ Decorated with @mcp.prompt
   â†’ Not relevant for your digest project
```

### How a Tool Is Defined (Conceptual, No Code)

A tool is simply a Python function with three things:
- A descriptive function name (this becomes the tool name)
- A docstring explaining what it does (this becomes the tool description that the LLM reads)
- Type annotations on all parameters and return value (FastMCP generates the schema from these)

The `@mcp.tool` decorator above the function is all that's needed to register it as an MCP tool. FastMCP handles everything else â€” the JSON schema, the parameter validation, the error handling, the protocol compliance.

### How Multiple Tools Go in One Server

You create one `FastMCP` server instance at the top of your file. Then every function with `@mcp.tool` below it becomes a tool in that server. There is no limit to how many tools one server can have. All tools share the same server instance, same dependencies, same deployment.

```
CONCEPTUAL STRUCTURE OF ONE SERVER FILE:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Create server instance: mcp = FastMCP("Research Server")

Tool 1: fetch_arxiv_papers    â†’ queries ArXiv API
Tool 2: fetch_arxiv_recent    â†’ gets papers from last 24h
Tool 3: search_papers_with_code â†’ queries PWC API
Tool 4: get_sota_benchmarks   â†’ gets PWC leaderboards
Tool 5: search_semantic_scholar â†’ queries S2 API
Tool 6: get_trending_citations â†’ gets top cited papers
Tool 7: get_conference_papers  â†’ queries OpenReview
Tool 8: fetch_rss_feed        â†’ fetches one RSS/Atom feed URL
Tool 9: fetch_all_feeds       â†’ fetches all 35+ configured feeds
Tool 10: fetch_url            â†’ fetches any URL as Markdown
Tool 11: search_kaggle_datasets â†’ queries Kaggle API

All 11 tools in ONE file, ONE server, ONE GitHub repo,
ONE Horizon deployment, ONE URL to call.
```

### Tool Naming Convention

Tools are organized using a namespace pattern so you can tell them apart:

```
NAMING PATTERN: category/action_name
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
arxiv/search             â†’ search ArXiv
arxiv/get_recent         â†’ get recent papers
papers/trending          â†’ trending on PWC
papers/sota              â†’ SOTA benchmarks
scholar/search           â†’ search Semantic Scholar
openreview/neurips       â†’ NeurIPS papers
openreview/icml          â†’ ICML papers
rss/fetch_feed           â†’ one RSS feed
rss/fetch_all            â†’ all configured feeds
fetch/url                â†’ any URL
kaggle/datasets          â†’ Kaggle datasets
kaggle/competitions      â†’ Kaggle competitions
reddit/r_machinelearning â†’ ML subreddit
reddit/r_localllama      â†’ LocalLLaMA subreddit
hn/top_ai                â†’ HN AI top stories
hf/trending_models       â†’ HF models
hf/daily_papers          â†’ HF daily papers
crawl/page               â†’ render any JS page
memory/save              â†’ save preference
search/web               â†’ web search
```

---

## 6. Server 1 â€” Research Server (Papers, RSS, Blogs)

### What This Server Covers

This is your heaviest server. It handles all research paper sources, all RSS/blog feeds, URL fetching, and dataset sources. Almost everything from Sections 1, 2, 7, 8, 9, 10 of your master list.

```
ai-digest-research-server
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
GitHub Repo:  github.com/YOUR_USERNAME/ai-digest-research-server
Horizon URL:  https://ai-digest-research.YOUR_NAME.fastmcp.app/mcp
Language:     Python
Framework:    FastMCP 3.0
Auth:         None for most tools (Kaggle needs free API token)
```

### Tools Inside This Server

```
TOOL GROUP 1: ArXiv Papers
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Tool: arxiv/search
  What it does:  Full-text search across all ArXiv papers
  Input params:  query keyword, category (cs.AI etc.), max results
  Returns:       List of papers with title, authors, abstract, URL, date
  Source API:    arxiv.org API (completely free, no auth)
  How to get:    pip install arxiv (official ArXiv Python library)
                 Docs: info.arxiv.org/help/api/index.html

Tool: arxiv/get_recent
  What it does:  Papers submitted in the last N days in a category
  Input params:  category, days_back (default: 1), max results
  Returns:       Same as above, sorted by submission date
  Source API:    Same ArXiv API, sorted by submittedDate


TOOL GROUP 2: Papers With Code
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Tool: papers/trending
  What it does:  Trending papers on Papers With Code today
  Input params:  max_results (default: 20)
  Returns:       Paper title, abstract, GitHub repo URL, paper URL
  Source API:    paperswithcode.com/api/v1/ (free, no auth needed)
  How to get:    Direct HTTP calls to PWC REST API
                 Docs: paperswithcode.com/api/v1/docs

Tool: papers/sota
  What it does:  Latest SOTA benchmark result updates
  Input params:  task (optional filter), max_results
  Returns:       Task name, benchmark name, best method, paper link
  Source API:    Same PWC API /results/ endpoint


TOOL GROUP 3: Semantic Scholar
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Tool: scholar/search
  What it does:  Search academic papers, get citation counts
  Input params:  query, fields (citations, abstract, year), max_results
  Returns:       Papers with title, authors, citations, abstract, URL
  Source API:    api.semanticscholar.org/graph/v1 (free, optional key)
  How to get:    Optional free API key from semanticscholar.org/product/api
                 Docs: api.semanticscholar.org/api-docs

Tool: scholar/trending_citations
  What it does:  Top-cited ML papers published in the last 7 days
  Input params:  min_citations, max_results
  Returns:       Papers sorted by recent citation velocity


TOOL GROUP 4: OpenReview (Conference Papers)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Tool: openreview/neurips
  What it does:  NeurIPS accepted papers
  Input params:  year, max_results, offset for pagination
  Returns:       Paper title, abstract, authors, forum URL
  Source API:    api2.openreview.net (free, needs free account)
  How to get:    Free account at openreview.net (just email + password)
                 Docs: docs.openreview.net/reference/api-v2

Tool: openreview/icml
  What it does:  ICML accepted papers
  Same structure as above, different venue ID

Tool: openreview/iclr
  What it does:  ICLR accepted papers
  Same structure as above, different venue ID

Tool: openreview/acl
  What it does:  ACL and EMNLP papers
  Same structure as above, multiple venue IDs

Tool: openreview/cvpr
  What it does:  CVPR computer vision papers
  Same structure as above, different venue ID


TOOL GROUP 5: RSS and Blog Feeds
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Tool: rss/fetch_feed
  What it does:  Fetches any single RSS or Atom feed URL
  Input params:  feed_url, max_items (default: 10)
  Returns:       List of items with title, link, summary, published date
  Library used:  feedparser (pip install feedparser)
                 feedparser.org â€” most reliable RSS parser for Python
  How to get:    pip install feedparser
                 Docs: feedparser.readthedocs.io

Tool: rss/fetch_all
  What it does:  Fetches ALL 35+ configured feeds simultaneously (async)
  Input params:  category filter (optional), max_items_per_feed
  Returns:       All items across all feeds in one merged list
  Config file:   rss_sources.yaml (lists all 35+ feed URLs with metadata)

ALL 35+ FEEDS CONFIGURED IN rss_sources.yaml:

  Company Blogs (all have RSS/Atom feeds):
  â”œâ”€â”€ Google AI Blog          ai.googleblog.com/feeds/posts/default
  â”œâ”€â”€ DeepMind Blog           deepmind.com/blog/rss.xml
  â”œâ”€â”€ OpenAI Blog             openai.com/blog/rss/
  â”œâ”€â”€ Anthropic Blog          anthropic.com/blog/rss
  â”œâ”€â”€ Meta AI / FAIR          research.facebook.com/blog/rss
  â”œâ”€â”€ Microsoft Research      microsoft.com/en-us/research/blog/feed/
  â”œâ”€â”€ NVIDIA Developer        blogs.nvidia.com/blog/feed/
  â”œâ”€â”€ HuggingFace Blog        huggingface.co/blog/feed
  â”œâ”€â”€ W&B Blog                wandb.ai/site/rss
  â”œâ”€â”€ AWS ML Blog             aws.amazon.com/blogs/machine-learning/feed/
  â”œâ”€â”€ Google Cloud AI         cloud.google.com/.../feed
  â”œâ”€â”€ Modal Labs Blog         (RSS feed URL)
  â”œâ”€â”€ Replicate Blog          (RSS feed URL)
  â”œâ”€â”€ Perplexity AI Blog      (RSS feed URL)
  â”œâ”€â”€ Together AI Blog        (RSS feed URL)
  â””â”€â”€ a16z AI Blog            (RSS feed URL)

  Agentic Framework Blogs:
  â”œâ”€â”€ LangChain Blog          blog.langchain.dev/feed.xml
  â”œâ”€â”€ LlamaIndex Blog         blog.llamaindex.ai/feed
  â””â”€â”€ MCP Blog                modelcontextprotocol.io/blog/feed.xml

  Newsletters:
  â”œâ”€â”€ The Batch               deeplearning.ai RSS feed
  â”œâ”€â”€ Import AI               Jack Clark newsletter RSS
  â”œâ”€â”€ BAIR Blog               bair.berkeley.edu/blog/feed.xml
  â”œâ”€â”€ Gradient Flow           Ben Lorica newsletter RSS
  â””â”€â”€ KDnuggets               kdnuggets.com/feed

  Backend Framework Releases (via GitHub Atom):
  â”œâ”€â”€ FastAPI                 github.com/tiangolo/fastapi/releases.atom
  â”œâ”€â”€ Flask                   github.com/pallets/flask/releases.atom
  â”œâ”€â”€ Django                  github.com/django/django/releases.atom
  â”œâ”€â”€ Streamlit               github.com/streamlit/streamlit/releases.atom
  â”œâ”€â”€ Gradio                  github.com/gradio-app/gradio/releases.atom
  â”œâ”€â”€ Ray                     github.com/ray-project/ray/releases.atom
  â”œâ”€â”€ MLflow                  github.com/mlflow/mlflow/releases.atom
  â”œâ”€â”€ BentoML                 github.com/bentoml/BentoML/releases.atom
  â”œâ”€â”€ Lightning               github.com/Lightning-AI/pytorch-lightning/releases.atom
  â”œâ”€â”€ Prefect                 github.com/PrefectHQ/prefect/releases.atom
  â””â”€â”€ Airflow                 github.com/apache/airflow/releases.atom

  Agentic Framework Releases (via GitHub Atom):
  â”œâ”€â”€ LangGraph               github.com/langchain-ai/langgraph/releases.atom
  â”œâ”€â”€ CrewAI                  github.com/joaomdmoura/crewai/releases.atom
  â”œâ”€â”€ AutoGen                 github.com/microsoft/autogen/releases.atom
  â”œâ”€â”€ LlamaIndex              github.com/jerryjliu/llama_index/releases.atom
  â”œâ”€â”€ Haystack                github.com/deepset-ai/haystack/releases.atom
  â”œâ”€â”€ OpenDevin               github.com/OpenDevin/OpenDevin/releases.atom
  â”œâ”€â”€ MetaGPT                 github.com/geekan/MetaGPT/releases.atom
  â”œâ”€â”€ Flowise                 github.com/FlowiseAI/Flowise/releases.atom
  â””â”€â”€ AutoGPT                 github.com/Significant-Gravitas/AutoGPT/releases.atom

  Aggregators and Directories:
  â””â”€â”€ AI Top Tools            aitoptools.com/feed

  Medium / Towards Data Science:
  â””â”€â”€ TDS AI/ML tag           medium.com/feed/tag/machine-learning


TOOL GROUP 6: URL Fetcher
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Tool: fetch/url
  What it does:  Fetches any URL and returns clean Markdown content
  Input params:  url, max_chars (default: 3000)
  Returns:       Clean readable text in Markdown format
  Use cases:     Pages without RSS (arXiv Sanity, HF Forums,
                 DataTau, OpenAI community, Anthropic community,
                 Futurepedia, any blog post URL)
  Library used:  httpx (pip install httpx) for HTTP calls
                 markdownify or html2text to convert HTML â†’ Markdown

Tool: fetch/arxiv_sanity
  What it does:  Fetches today's top papers from arxiv-sanity-lite
                 (Karpathy's curation tool, no official API)
  Returns:       Top papers from Karpathy's filtered ArXiv view
  Method:        fetch/url call to the arxiv-sanity-lite RSS/JSON export


TOOL GROUP 7: Kaggle
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Tool: kaggle/datasets
  What it does:  New ML-tagged datasets published recently
  Input params:  tags (["deep-learning", "nlp", "llm"]), days_back
  Returns:       Dataset name, description, size, download count, URL
  Source API:    Official Kaggle API (kaggle.com/docs/api)
  How to get:    pip install kaggle
                 Free account at kaggle.com â†’ Settings â†’ Create API Token
                 Downloads kaggle.json file with username + key

Tool: kaggle/competitions
  What it does:  Active ML competitions with prizes
  Input params:  category, days_since_deadline
  Returns:       Competition title, description, prize, deadline, URL
  Source API:    Same Kaggle API

Tool: kaggle/trending_notebooks
  What it does:  Trending notebooks on Kaggle (community insights)
  Input params:  topic, max_results
  Returns:       Notebook title, author, votes, URL
```

---

## 7. Server 2 â€” Community Server (Reddit, HN, HuggingFace)

### What This Server Covers

Community signals â€” what real people are discussing and finding interesting. Reddit, Hacker News, and the HuggingFace ecosystem (models trending, spaces popular, daily papers curated).

```
ai-digest-community-server
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
GitHub Repo:  github.com/YOUR_USERNAME/ai-digest-community-server
Horizon URL:  https://ai-digest-community.YOUR_NAME.fastmcp.app/mcp
Language:     Python
Framework:    FastMCP 3.0
Auth:         None needed for Reddit (public API)
              None needed for HN (public Algolia API)
              None for HuggingFace public data
```

### Tools Inside This Server

```
TOOL GROUP 1: Reddit
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Important note: These tools use Reddit's PUBLIC JSON endpoint
(reddit.com/r/subreddit/hot.json) â€” NO OAuth, NO API key,
NO Reddit account needed. This is a public endpoint Reddit
provides for read-only anonymous access.

Tool: reddit/r_machinelearning
  What it does:  Hot posts from r/MachineLearning
  Input params:  min_score (default: 100), max_posts (default: 20)
  Returns:       Post title, score, comment count, URL, text preview
  Source API:    reddit.com/r/MachineLearning/hot.json
                 No auth needed â€” completely public

Tool: reddit/r_localllama
  What it does:  Hot posts from r/LocalLLaMA
  Input params:  min_score (default: 50), max_posts
  Returns:       Same structure as above
  Source API:    reddit.com/r/LocalLLaMA/hot.json

Tool: reddit/r_deeplearning
  What it does:  Hot posts from r/DeepLearning
  Input params:  min_score, max_posts
  Source API:    reddit.com/r/DeepLearning/hot.json

Tool: reddit/r_datascience
  What it does:  Hot posts from r/DataScience
  Source API:    reddit.com/r/DataScience/hot.json

Tool: reddit/r_ai
  What it does:  Hot posts from r/ArtificialIntelligence
  Source API:    reddit.com/r/ArtificialIntelligence/hot.json

Tool: reddit/search
  What it does:  Search across all 5 subreddits for a keyword
  Input params:  query, subreddit (optional), time_filter, sort
  Returns:       Matching posts sorted by relevance or score
  Source API:    reddit.com/r/subreddit/search.json?q=query

Note on Stack Overflow: Stack Overflow has an official free API
(api.stackexchange.com) that can fetch trending AI/ML/LLM questions.
It is included as an optional tool in this server.

Tool: stackoverflow/trending_ai
  What it does:  Trending questions tagged ai/ml/llm on Stack Overflow
  Input params:  tags (["llm","langchain","fastapi"]), min_score
  Returns:       Question title, tags, answer count, view count, URL
  Source API:    api.stackexchange.com/2.3/questions
                 Completely free, 300 requests/day without key
                 10,000 requests/day with free key from stackapps.com


TOOL GROUP 2: Hacker News
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Tool: hn/top_ai
  What it does:  Top HN stories related to AI/ML today
  Input params:  min_points (default: 50), max_results, keywords
  Returns:       Story title, URL, points, comment count, HN link, time
  Source API:    hn.algolia.com/api/v1/search (Algolia HN Search API)
                 Completely free, no API key needed
  How to get:    No package needed â€” direct HTTP call to Algolia HN API
                 Docs: hn.algolia.com/api

Tool: hn/search
  What it does:  Search HN for specific keyword (e.g., "LangGraph")
  Input params:  query, date_range, min_points
  Returns:       Matching stories sorted by points
  Source API:    Same Algolia API with query parameter


TOOL GROUP 3: HuggingFace
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Note: HuggingFace has TWO options:
  Option A: Use HuggingFace's official MCP server (huggingface.co/mcp)
            â†’ Cloud-hosted by HF, move this to "already cloud-hosted" list
            â†’ Just use their URL, no tools to build

  Option B: Build the tools yourself in this server
            â†’ More control, but requires maintenance
            â†’ Use HF's free public REST API

We recommend Option A (official HF MCP, already cloud-hosted).
But if you want Option B, here are the tools:

Tool: hf/trending_models
  What it does:  Top 20 trending models by downloads today
  Input params:  pipeline_tag (text-generation etc.), max_results
  Returns:       Model ID, downloads, likes, tags, task type, URL
  Source API:    huggingface.co/api/models?sort=downloads&limit=20
                 Free, no auth for public models

Tool: hf/trending_spaces
  What it does:  Top 10 trending Spaces (demo apps) by likes
  Input params:  max_results
  Returns:       Space ID, likes, SDK type, description, URL
  Source API:    huggingface.co/api/spaces?sort=likes&limit=10

Tool: hf/daily_papers
  What it does:  HuggingFace curated daily papers (5-10/day)
  Input params:  date (default: today)
  Returns:       Paper title, abstract, upvotes, URL
  Source API:    huggingface.co/api/daily_papers

Tool: hf/leaderboard
  What it does:  Latest ChatbotArena / LMSYS leaderboard rankings
  Input params:  top_n (default: 20)
  Returns:       Model name, arena score, rank change, organization
  Source API:    huggingface.co API

Tool: hf/datasets
  What it does:  New ML datasets added to the Hub
  Input params:  task_categories, max_results, days_back
  Returns:       Dataset ID, description, downloads, size, URL
  Source API:    huggingface.co/api/datasets
```

---

## 8. Server 3 â€” Utility Server (Crawl, Memory, Search)

### What This Server Covers

Supporting tools for the pipeline. Not data sources themselves, but tools that help fetch, process, remember, and search.

```
ai-digest-utility-server
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
GitHub Repo:  github.com/YOUR_USERNAME/ai-digest-utility-server
Horizon URL:  https://ai-digest-utility.YOUR_NAME.fastmcp.app/mcp
Language:     Python
Framework:    FastMCP 3.0
Special:      Crawl4AI needs Playwright browser â€” this server
              is slightly heavier than the others
```

### Tools Inside This Server

```
TOOL GROUP 1: Crawl4AI â€” Full JS Browser Rendering
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Tool: crawl/page
  What it does:  Renders any webpage with full JavaScript execution
                 Returns clean Markdown content
  Why needed:    Some sites (Medium, Futurepedia, some JS-heavy blogs)
                 block simple HTTP fetch requests or require JS to load
                 their content. Simple fetch/url won't work on these.
  Input params:  url, wait_for_selector (optional), max_chars
  Returns:       Full page content as clean Markdown
  Library:       crawl4ai (pip install crawl4ai)
                 GitHub: github.com/unclecode/crawl4ai
                 Uses Playwright browser internally
  Install extra: After pip install, run: playwright install chromium

Tool: crawl/blog_post
  What it does:  Specifically optimized for blog post extraction
                 Removes navigation, ads, footers, returns only article
  Input params:  url
  Returns:       Article title, author, date, full content, tags

Tool: crawl/batch
  What it does:  Crawl a list of URLs in parallel
  Input params:  urls (list), max_concurrent (default: 5)
  Returns:       List of results in same order as input URLs

SOURCES THIS COVERS THAT server-fetch/url CANNOT:
  â†’ Medium / Towards Data Science (JS-heavy)
  â†’ Futurepedia (dynamic content)
  â†’ Apple Machine Learning Research Blog (no RSS)
  â†’ Some community forum pages
  â†’ Any JS-rendered page


TOOL GROUP 2: SearXNG â€” Private Web Search
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Tool: search/web
  What it does:  Private web search with no API key needed
                 Uses SearXNG metasearch engine
  Why needed:    For Telegram bot's /search command â€” when users ask
                 about something not yet in the Supabase database
  Input params:  query, categories, max_results
  Returns:       Search results with title, URL, snippet, source
  How it works:  SearXNG is an open-source metasearch engine that
                 aggregates results from Google, Bing, DuckDuckGo
                 and many others without tracking
  GitHub:        github.com/searxng/searxng
  Instance:      You can use a public SearXNG instance (many exist free)
                 OR deploy your own to Horizon (recommended for privacy)
  Public list:   searx.space (list of public free instances)


TOOL GROUP 3: Memory â€” Persistent Preferences
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Tool: memory/save
  What it does:  Save a user preference, starred item, or note
  Input params:  key, value, category
  Returns:       Confirmation
  Storage:       Simple file or SQLite on Horizon (persistent)
  Use case:      "Remember I always want LangChain items first"
                 "Star this paper for follow-up"

Tool: memory/recall
  What it does:  Retrieve saved preferences or starred items
  Input params:  key or category
  Returns:       Stored value
  Use case:      Bot personalizes output based on your preferences

Tool: memory/list
  What it does:  List all saved memory entries
  Input params:  category filter (optional)
  Returns:       All saved key-value pairs
```

---

## 9. Already Cloud-Hosted MCPs â€” Zero Setup Needed

These are official MCP servers hosted by the companies themselves. You do not write any code for these. You do not deploy anything. You just add their URL to your GitHub Actions config and call them directly.

### GitHub MCP Server (Official by GitHub)

```
Who hosts it:   GitHub's own servers
URL to use:     https://api.githubcopilot.com/mcp/
Auth:           Your free GitHub account (OAuth)
Cost:           Free with any GitHub account

What it gives you:
  â†’ Search GitHub for trending repos by topic
    Topics to monitor: llm, ai, agents, rag, mcp, langchain,
                       transformers, neural-network, deep-learning
  â†’ Monitor specific repo releases (all 22 framework repos)
  â†’ Get release notes and changelogs
  â†’ Browse issues, PRs, README content
  â†’ Search code across GitHub

Frameworks it monitors releases for:
  Agentic:   LangGraph, CrewAI, AutoGen, LlamaIndex,
             Haystack, OpenDevin, MetaGPT, Flowise, AutoGPT
  Backend:   FastAPI, Flask, Django, Starlette, Streamlit,
             Gradio, Ray, MLflow, BentoML, Lightning,
             Prefect, Airflow, LangServe, Chainlit
  All 22+ repos covered by one MCP server

Documentation: docs.github.com/en/copilot/building-copilot-extensions/
               building-a-copilot-mcp-server
```

### HuggingFace MCP Server (Official by HuggingFace)

```
Who hosts it:   HuggingFace's own servers
URL to use:     https://huggingface.co/mcp
Auth:           Free HuggingFace account login
Cost:           Free

What it gives you:
  â†’ Trending models (by downloads and likes)
  â†’ Trending Spaces (demo apps)
  â†’ Daily Papers (curated by HF team)
  â†’ Datasets Hub (new datasets)
  â†’ Model comparisons
  â†’ Leaderboard data (ChatbotArena)
  â†’ More as HF expands the server

Note: If you build hf/* tools in Server 2 yourself, you do not
need this. Using the official server is easier and lower maintenance.
```

### Supabase MCP Server (Official by Supabase)

```
Who hosts it:   Supabase's own servers
URL to use:     https://mcp.supabase.com/mcp?project_ref=YOUR_REF
Auth:           Your Supabase project reference (from project settings)
Cost:           Free (it's your own database being queried)

What it gives you:
  â†’ Query your digest database using natural language
  â†’ Insert, update, delete records
  â†’ Manage tables and indexes
  â†’ Check logs and errors
  â†’ The Telegram bot can query your DB through this

Documentation: github.com/supabase-community/supabase-mcp
```

### Context7 MCP (Hosted by Context7)

```
Who hosts it:   Context7 (upstash/context7-mcp)
How to use:     npx -y @upstash/context7-mcp (connects to cloud)
Auth:           None required
Cost:           Free tier (generous for personal use)

What it gives you:
  â†’ Up-to-date documentation for any library
  â†’ When your bot receives /docs fastapi or /whatsnew langgraph
    it calls Context7 which returns CURRENT docs, not LLM training data
  â†’ Resolves library name to documentation
  â†’ Returns specific version docs
  â†’ Much better than asking an LLM about docs from its training data

GitHub:        github.com/upstash/context7
```

---

## 10. Complete Master Source Coverage Map

Every single item from your master source document, mapped to the exact tool that covers it:

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
SECTION 1: RESEARCH & PAPERS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Source                      Tool                    Server        Cost
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ArXiv cs.LG, cs.AI, stat.ML  arxiv/search            Research-S1   FREE
ArXiv cs.CL, cs.CV           arxiv/get_recent        Research-S1   FREE
arXiv Sanity (Karpathy)      fetch/url               Research-S1   FREE
                             (fetches arxiv-sanity-lite.org)
Semantic Scholar AI feed     scholar/search          Research-S1   FREE
Semantic Scholar citations   scholar/trending_cites  Research-S1   FREE
Papers With Code trending    papers/trending         Research-S1   FREE
Papers With Code SOTA        papers/sota             Research-S1   FREE
Papers With Code datasets    papers/datasets         Research-S1   FREE
NeurIPS proceedings          openreview/neurips      Research-S1   FREE
ICML proceedings             openreview/icml         Research-S1   FREE
ICLR proceedings             openreview/iclr         Research-S1   FREE
CVPR proceedings             openreview/cvpr         Research-S1   FREE
ACL proceedings              openreview/acl          Research-S1   FREE
EMNLP proceedings            openreview/acl          Research-S1   FREE
OpenReview API               (all openreview/* tools) Research-S1  FREE
Academic datasets (Kaggle)   kaggle/datasets         Research-S1   FREE

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
SECTION 2: NEWSLETTERS & BLOGS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Source                      Tool                    Server        Cost
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
The Batch (deeplearning.ai)  rss/fetch_all           Research-S1   FREE
Import AI (Jack Clark)       rss/fetch_all           Research-S1   FREE
BAIR Blog (UC Berkeley)      rss/fetch_all           Research-S1   FREE
Gradient Flow (Ben Lorica)   rss/fetch_all           Research-S1   FREE
Yannic Kilcher's summaries   âš ï¸ YouTube RSS          Research-S1   FREE
                             (youtube.com/@YannicKilcher RSS exists)
HuggingFace Blog             rss/fetch_all           Research-S1   FREE
W&B Blog                     rss/fetch_all           Research-S1   FREE
AWS ML Blog                  rss/fetch_all           Research-S1   FREE
Google Cloud AI Blog         rss/fetch_all           Research-S1   FREE
Microsoft Azure AI Blog      rss/fetch_all           Research-S1   FREE
Medium / Towards Data Science rss/fetch_all +        Research-S1   FREE
                              crawl/page (for JS)    Utility-S3
KDnuggets News               rss/fetch_all           Research-S1   FREE

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
SECTION 3: CODE, TOOLS & FRAMEWORKS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Source                      Tool                    Server        Cost
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
HuggingFace Model Hub        hf/trending_models      Community-S2  FREE
                             OR HF official MCP      Cloud MCP     FREE
HuggingFace Spaces           hf/trending_spaces      Community-S2  FREE
Papers With Code SOTA        papers/sota             Research-S1   FREE
PyPI trending packages       rss/fetch_feed          Research-S1   FREE
                             (pypi.org/rss/updates.xml)
Conda new ML packages        rss/fetch_feed          Research-S1   FREE
                             (anaconda.org RSS)
Kaggle Datasets              kaggle/datasets         Research-S1   FREE
Kaggle Competitions          kaggle/competitions     Research-S1   FREE
GitHub Trending AI/ML        github-mcp (search)     Cloud MCP     FREE
GitHub Releases (frameworks) github-mcp (releases)   Cloud MCP     FREE
FastAPI releases             rss/fetch_all           Research-S1   FREE
                             (GitHub .atom feed)
Flask releases               rss/fetch_all           Research-S1   FREE
Django releases              rss/fetch_all           Research-S1   FREE
Lightning AI releases        rss/fetch_all           Research-S1   FREE

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
SECTION 4: AGENTIC & DEVELOPER FRAMEWORKS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Source                      Tool                    Server        Cost
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
LangChain Blog               rss/fetch_all           Research-S1   FREE
LlamaIndex Blog              rss/fetch_all           Research-S1   FREE
CrewAI GitHub releases       rss/fetch_all +         Research-S1   FREE
                             github-mcp              Cloud MCP     FREE
MCP Blog                     rss/fetch_all           Research-S1   FREE
AutoGen (Microsoft)          rss/fetch_all +         Research-S1   FREE
                             github-mcp              Cloud MCP     FREE
Haystack (Deepset)           rss/fetch_all +         Research-S1   FREE
                             github-mcp              Cloud MCP     FREE
Flowise                      rss/fetch_all +         Research-S1   FREE
                             github-mcp              Cloud MCP     FREE
AutoGPT / GPT Engineer       rss/fetch_all +         Research-S1   FREE
                             github-mcp              Cloud MCP     FREE
OpenDevin / MetaGPT          github-mcp              Cloud MCP     FREE

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
SECTION 5: COMMUNITY & DISCUSSIONS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Source                      Tool                    Server        Cost
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
r/MachineLearning            reddit/r_machinelearning Community-S2  FREE
r/DeepLearning               reddit/r_deeplearning   Community-S2  FREE
r/LocalLLaMA                 reddit/r_localllama     Community-S2  FREE
r/DataScience                reddit/r_datascience    Community-S2  FREE
r/ArtificialIntelligence     reddit/r_ai             Community-S2  FREE
Hacker News AI stories       hn/top_ai               Community-S2  FREE
Stack Overflow AI trends     stackoverflow/trending  Community-S2  FREE
HuggingFace Forums           fetch/url               Research-S1   FREE
OpenAI community posts       fetch/url               Research-S1   FREE
Anthropic community posts    fetch/url               Research-S1   FREE
LinkedIn posts               âŒ NO API AVAILABLE     SKIP          â€”
                             (ToS violation, no public API)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
SECTION 6: VIDEOS & PODCASTS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Source                      Tool                    Server        Cost
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
YouTube channels (all)       rss/fetch_feed          Research-S1   FREE
                             YouTube provides RSS for every channel:
                             youtube.com/feeds/videos.xml?channel_id=ID
                             Gives titles, descriptions, upload dates
                             (No video content, just metadata)
Yannic Kilcher YouTube       rss/fetch_feed (YT RSS) Research-S1   FREE
Two Minute Papers            rss/fetch_feed (YT RSS) Research-S1   FREE
Sentdex                      rss/fetch_feed (YT RSS) Research-S1   FREE
CodeEmporium                 rss/fetch_feed (YT RSS) Research-S1   FREE
HuggingFace YouTube          rss/fetch_feed (YT RSS) Research-S1   FREE
OpenAI YouTube               rss/fetch_feed (YT RSS) Research-S1   FREE
Podcasts (all)               rss/fetch_feed          Research-S1   FREE
                             All podcasts have RSS feeds.
                             Gives episode title, description, date.
Gradient Dissent (W&B)       rss/fetch_feed          Research-S1   FREE
Lex Fridman Podcast          rss/fetch_feed          Research-S1   FREE
ML Street Talk               rss/fetch_feed          Research-S1   FREE
TWiML AI Podcast             rss/fetch_feed          Research-S1   FREE
Practical AI Podcast         rss/fetch_feed          Research-S1   FREE

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
SECTION 7: COMPANY & RESEARCH BLOGS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
All have RSS feeds â†’ covered by rss/fetch_all in Research-S1   FREE

Apple ML Research Blog       crawl/page              Utility-S3    FREE
(no RSS â€” needs JS crawling)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
SECTION 8: AGGREGATORS & DIRECTORIES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Source                      Tool                    Server        Cost
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
AI Top Tools                 rss/fetch_all           Research-S1   FREE
Futurepedia                  crawl/page              Utility-S3    FREE
AI Papers Daily (Telegram)   fetch/url               Research-S1   âš ï¸PARTIAL
                             (Telegram channels not easily fetchable)
HuggingFace Daily Papers     hf/daily_papers         Community-S2  FREE
DataTau                      fetch/url               Research-S1   FREE
arXiv Digest channels        rss/fetch_feed          Research-S1   FREE
                             (Substack newsletters have RSS)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
SECTION 9: BACKEND & INFRA ECOSYSTEM
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
All GitHub releases covered by github-mcp (Cloud MCP) or
rss/fetch_all (GitHub Atom feeds in Research-S1)        FREE

LangServe / Chainlit         rss/fetch_all +         Research-S1   FREE
                             github-mcp              Cloud MCP     FREE

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
SECTION 10: ADDITIONAL / OPTIONAL
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Source                      Tool                    Server        Cost
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
HF Datasets Hub              hf/datasets             Community-S2  FREE
HF Leaderboard               hf/leaderboard          Community-S2  FREE
Kaggle Trending Notebooks    kaggle/trending_ntbks   Research-S1   FREE
Semantic Scholar citations   scholar/trending_cites  Research-S1   FREE
Modal Labs Blog              rss/fetch_all           Research-S1   FREE
Replicate Blog               rss/fetch_all           Research-S1   FREE
Perplexity AI Blog           rss/fetch_all           Research-S1   FREE
Together AI Blog             rss/fetch_all           Research-S1   FREE
a16z AI Blog                 rss/fetch_all           Research-S1   FREE
Sequoia Generative AI Blog   rss/fetch_all           Research-S1   FREE
AI Snakepit (ethics)         rss/fetch_feed          Research-S1   FREE
Gradient Institute           rss/fetch_feed          Research-S1   FREE
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

OVERALL COVERAGE: 98% of all sources âœ…
UNCOVERABLE:  LinkedIn (ToS), Telegram private channels
COST: $0
```

---

## 11. How to Deploy to Prefect Horizon (Step by Step)

This section explains the full deployment process for each of your 3 servers. The process is identical for all three. You do it once per server.

### Prerequisites Before Deploying

```
1. GitHub account (free at github.com)
2. Prefect Horizon account (free at horizon.prefect.io)
   â†’ Sign in with your GitHub account (no separate registration)
3. FastMCP installed locally for testing
   â†’ pip install fastmcp
4. Python 3.11+ installed on your PC (for local development/testing only)
   â†’ Once deployed, PC is not needed
```

### Step-by-Step Process

```
PHASE 1 â€” LOCAL DEVELOPMENT
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

STEP 1: Create a new folder for the server
  Name it: ai-digest-research-server
  (or community-server, or utility-server)

STEP 2: Create the main Python file
  Name it: main.py
  This file will contain:
    - One FastMCP server instance at the top
    - One Python function per tool
    - @mcp.tool decorator above each function
    - A docstring explaining each tool (this is what the pipeline reads)

STEP 3: Import the libraries your tools need
  All library imports go at the top of main.py
  Common imports for Research Server:
    - feedparser (for RSS parsing)
    - arxiv (official ArXiv Python library)
    - httpx (for async HTTP requests to REST APIs)
    - html2text or markdownify (for HTML â†’ Markdown conversion)
  Common for Community Server:
    - httpx (for Reddit JSON and HN Algolia calls)
  Common for Utility Server:
    - crawl4ai (for JS browser rendering)

STEP 4: Create requirements.txt
  List every Python library your server needs, one per line
  Example for Research Server:
    feedparser
    arxiv
    httpx
    html2text
    kaggle
    fastmcp
  Horizon reads this file automatically and installs everything

STEP 5: Create config/rss_sources.yaml (Research Server only)
  This YAML file lists all 35+ RSS feed URLs
  Organized by category: company_blogs, newsletters, framework_releases
  The rss/fetch_all tool reads this file to know which feeds to fetch

STEP 6: Test locally with FastMCP Inspector
  Run: fastmcp dev main.py
  Opens a browser at localhost:6274
  You can click on each tool, enter parameters, and see the output
  Test every single tool before deploying
  Fix any errors before moving to Phase 2


PHASE 2 â€” GITHUB REPOSITORY
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

STEP 7: Create a new GitHub repository
  Go to github.com â†’ New repository
  Name: ai-digest-research-server (or community/utility)
  Set to Private (recommended â€” hides your API keys in config)
  Initialize with README

STEP 8: Push your code
  Upload these files to the repo:
    main.py
    requirements.txt
    config/rss_sources.yaml (Research Server only)
    .env.example (template listing needed env vars, no actual values)
  DO NOT commit .env with actual API keys
  Use GitHub Secrets for real keys (added in Repo Settings â†’ Secrets)

STEP 9: Add secrets to GitHub repository
  Go to: Your repo â†’ Settings â†’ Secrets and Variables â†’ Actions
  Add each secret your server needs:
    Research Server:  KAGGLE_USERNAME, KAGGLE_KEY, SEMANTIC_SCHOLAR_KEY
    Community Server: No secrets needed (all public APIs)
    Utility Server:   No secrets needed


PHASE 3 â€” PREFECT HORIZON DEPLOYMENT
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

STEP 10: Go to horizon.prefect.io
  Sign in with your GitHub account
  (same account where your repos live)

STEP 11: Click "Deploy New Server"
  Horizon shows all your GitHub repos
  Select: ai-digest-research-server
  (repeat for community and utility servers)

STEP 12: Configure the deployment
  Fill in these fields:
    Server name:  ai-digest-research (becomes part of your URL)
    Description:  "Research papers, RSS feeds, blogs for AI digest"
    Entrypoint:   main.py:mcp
                  (tells Horizon which file and which object to run)
    Python version: 3.11 (recommended)
    Authentication: OFF for personal use (you control access via token)

STEP 13: Set environment variables in Horizon
  Horizon has a Secrets/Variables section in each deployment
  Add your real API keys here:
    KAGGLE_USERNAME = your_kaggle_username
    KAGGLE_KEY = your_kaggle_api_key
    SEMANTIC_SCHOLAR_KEY = your_optional_key
  These are injected at runtime â€” never exposed in code

STEP 14: Click Deploy
  Horizon:
    â†’ Clones your GitHub repo
    â†’ Installs from requirements.txt
    â†’ Starts your FastMCP server
    â†’ Gives you a live HTTPS URL
  Takes approximately 60 seconds

STEP 15: Copy your server URL
  It will look like:
    https://ai-digest-research.YOUR_HORIZON_USERNAME.fastmcp.app/mcp
  Save this URL â€” you'll put it in your GitHub Actions pipeline config

STEP 16: Test the live URL
  In Horizon, click "ChatMCP" to open the test interface
  Try calling each tool with real parameters
  Verify results look correct
  If something's wrong, fix it in your local code and push to GitHub
  Horizon auto-redeploys within seconds of every push

REPEAT STEPS 10-16 FOR EACH OF THE 3 SERVERS
```

### What Horizon Gives You Automatically (No Extra Work)

```
CI/CD:        Every git push to your repo â†’ Horizon auto-redeploys
              Branch previews for testing changes without affecting prod
Monitoring:   Real-time logs, error tracking, call history
Scaling:      Horizon handles traffic spikes automatically
Security:     OAuth 2.1 built in (optional for personal use)
Rollbacks:    One click to revert to any previous version
Uptime:       24/7 â€” not dependent on your PC at all
```

---

## 12. GitHub Actions â€” The Free 24/7 Cron Trigger

GitHub Actions is the cloud scheduler that wakes up every day at 7 AM IST and runs your pipeline. It runs on GitHub's own cloud servers â€” your PC does not need to be running.

### How It Works

```
YOUR GITHUB REPO: ai-digest-pipeline
  â†“
Contains file: .github/workflows/daily_digest.yml
  â†“
GitHub reads this file and schedules it
  â†“
Every day at 1:30 AM UTC (7:00 AM IST):
  GitHub spins up an Ubuntu container on their servers
  The container runs for ~3-5 minutes
  The container calls your Horizon MCP servers
  The container calls Gemini API
  The container saves to Supabase
  The container sends notifications
  The container shuts down automatically
  â†“
You wake up to a complete digest on all channels
```

### What the Workflow File Does (Plain English)

```
.github/workflows/daily_digest.yml â€” what it defines:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

TRIGGER:
  Schedule: Run at 1:30 AM UTC every day (7:00 AM IST)
  Manual: Also allow clicking "Run Workflow" button in GitHub UI
          (useful for testing without waiting for 7 AM)

STEPS:
  1. Check out your repository code
  2. Set up Python 3.11 environment
  3. Cache pip dependencies (speeds up future runs)
  4. Install Python packages from requirements.txt
  5. Run the main pipeline script (orchestrator.py)
     with all environment variables injected from GitHub Secrets

SECRETS NEEDED (add to GitHub repo â†’ Settings â†’ Secrets):
  RESEARCH_SERVER_URL     â†’ your Horizon research server URL
  COMMUNITY_SERVER_URL    â†’ your Horizon community server URL
  UTILITY_SERVER_URL      â†’ your Horizon utility server URL
  GITHUB_MCP_URL          â†’ api.githubcopilot.com/mcp/
  GITHUB_TOKEN            â†’ your GitHub PAT for GitHub MCP auth
  SUPABASE_MCP_URL        â†’ mcp.supabase.com URL with project ref
  SUPABASE_KEY            â†’ your Supabase service role key
  GEMINI_API_KEY          â†’ your Gemini Flash 2.0 API key
  TELEGRAM_BOT_TOKEN      â†’ your Telegram bot token
  TELEGRAM_CHAT_ID        â†’ your personal Telegram chat ID
  DISCORD_WEBHOOK_URL     â†’ your Discord webhook URL
  RESEND_API_KEY          â†’ your Resend email API key
  EMAIL_TO                â†’ your personal email address

FREE USAGE CHECK:
  Pipeline runs ~4 minutes per day
  4 min Ã— 30 days = 120 minutes/month
  GitHub free tier = 2,000 minutes/month
  Budget used: 6% â€” extremely safe
```

---

## 13. Complete Pipeline Flow (PC Off, Everything Automated)

This is the full minute-by-minute flow of what happens every morning:

```
7:00:00 AM IST â€” GitHub Actions wakes up on GitHub's cloud
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

7:00:05 AM â€” GitHub Actions container starts
  â†’ Checks out your pipeline code
  â†’ Installs dependencies (fast due to caching)
  â†’ Reads all MCP server URLs from secrets

7:00:30 AM â€” PARALLEL DATA COLLECTION BEGINS
  All 4 MCP servers called simultaneously:

  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Research Server (Horizon)                           â”‚
  â”‚   â†’ arxiv/get_recent(cs.AI, days=1)     ~50 papers  â”‚
  â”‚   â†’ arxiv/get_recent(cs.LG, days=1)     ~50 papers  â”‚
  â”‚   â†’ papers/trending(max=20)             ~20 papers  â”‚
  â”‚   â†’ scholar/trending_cites              ~15 papers  â”‚
  â”‚   â†’ openreview/neurips                  ~10 papers  â”‚
  â”‚   â†’ rss/fetch_all                       ~150 blogs  â”‚
  â”‚   â†’ kaggle/datasets                     ~10 items   â”‚
  â”‚   Total: ~305 items                                 â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Community Server (Horizon)                          â”‚
  â”‚   â†’ reddit/r_machinelearning            ~20 posts   â”‚
  â”‚   â†’ reddit/r_localllama                 ~20 posts   â”‚
  â”‚   â†’ reddit/r_deeplearning               ~15 posts   â”‚
  â”‚   â†’ reddit/r_datascience                ~15 posts   â”‚
  â”‚   â†’ reddit/r_ai                         ~15 posts   â”‚
  â”‚   â†’ hn/top_ai                           ~20 stories â”‚
  â”‚   â†’ hf/trending_models                  ~20 models  â”‚
  â”‚   â†’ hf/daily_papers                     ~8 papers   â”‚
  â”‚   â†’ hf/trending_spaces                  ~10 spaces  â”‚
  â”‚   Total: ~143 items                                 â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ GitHub MCP (GitHub's cloud)                         â”‚
  â”‚   â†’ search repos by topic (llm, ai, agents...)      â”‚
  â”‚   â†’ check all 22 framework repos for new releases   â”‚
  â”‚   Total: ~40 items                                  â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Utility Server (Horizon)                            â”‚
  â”‚   â†’ crawl/page for JS-heavy blogs                   â”‚
  â”‚   â†’ Used selectively, not every run                 â”‚
  â”‚   Total: ~10-15 items                               â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  TOTAL RAW ITEMS COLLECTED: ~490-500 items

7:01:30 AM â€” DEDUPLICATION
  Input:  ~500 raw items
  Stage 1 URL dedup:    removes 120 exact URL duplicates
  Stage 2 Fuzzy titles: removes 120 near-duplicates
  Output: ~250 unique items

7:01:35 AM â€” GEMINI FLASH 2.0 CALL
  Single batch request to Gemini API
  All 250 items in one JSON payload
  Gemini processes everything simultaneously
  Returns for each item:
    â†’ 2-sentence technical summary
    â†’ Relevance score 1-10
    â†’ is_breaking flag
    â†’ Tags array
    â†’ Framework mentions array
  Token usage: ~50,000 tokens (5% of 1M limit)
  API calls used: 1 (of 1,500 free daily limit)

7:02:30 AM â€” SUPABASE SAVE
  Items sorted by relevance score (highest first)
  Saved to 3 tables:
    news_items:    ~160 blog + community items
    papers:        ~55 research papers
    github_repos:  ~40 repos and releases
  digest_runs table: 1 audit log entry

7:02:45 AM â€” PARALLEL PUBLISHING
  All 3 channels publish simultaneously:

  ğŸ“§ Resend Email:
    Top 10 news items
    Top 5 framework updates
    Top 3 research papers
    Sent to your email

  ğŸ¤– Telegram Bot:
    Breaking news section (if any)
    Framework updates section
    Top stories section
    GitHub trending section
    Research papers section

  ğŸ® Discord Webhook:
    Color-coded embeds per category
    Breaking news in red
    Agents in green, Backend in blue

7:03:30 AM â€” DONE
  Dashboard on Vercel auto-shows latest data
  (reads Supabase directly â€” no action needed)

  YOUR PC = OFF THE ENTIRE TIME âœ…
  YOU WAKE UP TO COMPLETE DIGEST âœ…

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Total pipeline time: ~3.5 minutes
Total cost: $0
```

---

## 14. Project Folder Structure â€” All 3 Repos

```
REPO 1: ai-digest-research-server/
â”‚
â”œâ”€â”€ main.py                          â† FastMCP server with ALL tools
â”‚                                       One FastMCP instance at top
â”‚                                       11 tool functions below it
â”‚                                       Each function = one @mcp.tool
â”‚
â”œâ”€â”€ requirements.txt                 â† Python dependencies
â”‚                                       feedparser, arxiv, httpx,
â”‚                                       html2text, kaggle, fastmcp
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ rss_sources.yaml             â† ALL 35+ RSS feed URLs
â”‚   â”‚                                   Organized by category
â”‚   â”‚                                   Loaded by rss/fetch_all tool
â”‚   â”‚
â”‚   â””â”€â”€ conferences.yaml             â† OpenReview venue IDs
â”‚                                       NeurIPS, ICML, ICLR, ACL, CVPR
â”‚
â”œâ”€â”€ .env.example                     â† Template for needed keys
â”‚                                       KAGGLE_USERNAME=
â”‚                                       KAGGLE_KEY=
â”‚                                       SEMANTIC_SCHOLAR_KEY=
â”‚
â””â”€â”€ README.md                        â† Documentation for this server
                                        Tool list, usage examples


REPO 2: ai-digest-community-server/
â”‚
â”œâ”€â”€ main.py                          â† FastMCP server with community tools
â”‚                                       Reddit, HN, HuggingFace tools
â”‚                                       StackOverflow optional tool
â”‚
â”œâ”€â”€ requirements.txt                 â† httpx, fastmcp
â”‚                                       (No special auth libraries needed)
â”‚
â””â”€â”€ README.md


REPO 3: ai-digest-utility-server/
â”‚
â”œâ”€â”€ main.py                          â† FastMCP server with utility tools
â”‚                                       crawl4ai, SearXNG, memory tools
â”‚
â”œâ”€â”€ requirements.txt                 â† crawl4ai, fastmcp, sqlitedict
â”‚                                       (crawl4ai installs playwright)
â”‚
â”œâ”€â”€ searxng_instance.txt             â† URL of your chosen SearXNG instance
â”‚                                       (public instance or your own)
â”‚
â””â”€â”€ README.md


REPO 4: ai-digest-pipeline/          â† THE MAIN PIPELINE REPO
â”‚                                       This is what GitHub Actions runs
â”‚
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ daily_digest.yml         â† Cron schedule + workflow steps
â”‚
â”œâ”€â”€ pipeline/
â”‚   â”œâ”€â”€ orchestrator.py              â† Calls all 7 MCP server URLs
â”‚   â”‚                                   Collects all items
â”‚   â”‚                                   Runs deduplication
â”‚   â”‚                                   Calls Gemini
â”‚   â”‚                                   Calls Supabase
â”‚   â”‚                                   Triggers publishers
â”‚   â”‚
â”‚   â”œâ”€â”€ deduplicator.py              â† URL hash + fuzzy title matching
â”‚   â””â”€â”€ gemini_client.py             â† Batch call to Gemini Flash 2.0
â”‚
â”œâ”€â”€ publishers/
â”‚   â”œâ”€â”€ telegram_publisher.py        â† Sends to Telegram bot
â”‚   â”œâ”€â”€ discord_publisher.py         â† Sends Discord webhook embeds
â”‚   â””â”€â”€ email_publisher.py           â† Sends email via Resend
â”‚
â”œâ”€â”€ mcp_config.py                    â† All 7 MCP server URLs
â”‚                                       Loaded from environment variables
â”‚
â”œâ”€â”€ requirements.txt                 â† fastmcp, httpx, resend, etc.
â”‚
â””â”€â”€ .env.example                     â† All required secret names
```

---

## 15. How Each Tool Works â€” Source by Source

### RSS and Blog Feeds â€” How feedparser Works

```
HOW feedparser WORKS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
feedparser is a Python library that reads RSS/Atom feed URLs
and converts them into Python objects you can work with.

You give it a URL like:
  https://openai.com/blog/rss/

It returns a structured object with:
  feed.title       â†’ "OpenAI Blog"
  feed.entries     â†’ list of articles
  entry.title      â†’ "OpenAI releases o3"
  entry.link       â†’ "https://openai.com/blog/o3"
  entry.summary    â†’ first 500 characters of article
  entry.published  â†’ "2026-02-23T08:00:00Z"

Your rss/fetch_all tool:
  1. Reads rss_sources.yaml to get all 35+ URLs
  2. Calls feedparser on ALL of them simultaneously (async)
  3. Takes the 10 most recent entries from each feed
  4. Returns everything as a flat list of ScrapedItem objects

Why GitHub release Atom feeds work perfectly:
  github.com/tiangolo/fastapi/releases.atom
  â†“
  feedparser reads it
  â†“
  entry.title    â†’ "0.111.0"
  entry.link     â†’ release URL
  entry.summary  â†’ release notes text
  entry.published â†’ exact release datetime
  â†“
  PERFECT data for framework release tracking
```

### ArXiv Tool â€” How the arxiv Library Works

```
HOW arxiv Python Library WORKS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
The arxiv Python library (pip install arxiv) wraps the
official ArXiv API in a clean Python interface.

Official ArXiv API documentation: info.arxiv.org/help/api/index.html
Python library docs:              lukasschwab.me/arxiv.py/

Your tool:
  1. Receives: category="cs.AI", days_back=1, max_results=50
  2. Calls ArXiv API with:
     â†’ search_query: "cat:cs.AI"
     â†’ sortBy: submittedDate
     â†’ dateFrom: yesterday's date
  3. Returns each paper with:
     â†’ title, authors (list), abstract, arxiv_url, pdf_url
     â†’ submitted date, primary category, all categories
  4. All completely free, no authentication, no rate limits
```

### Reddit Tools â€” No OAuth Needed

```
HOW REDDIT PUBLIC JSON API WORKS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Reddit provides a completely open JSON endpoint for every subreddit.
No authentication. No API key. No OAuth. Just a direct HTTP GET.

URL pattern:
  reddit.com/r/SUBREDDIT_NAME/hot.json?limit=25

Returns JSON with:
  data.children â†’ list of posts
  Each post:
    data.title       â†’ post headline
    data.url         â†’ linked URL (or reddit URL for text posts)
    data.score       â†’ upvote count
    data.permalink   â†’ reddit.com/r/.../comments/...
    data.selftext    â†’ text content (if it's a text post)
    data.num_comments â†’ comment count
    data.created_utc â†’ unix timestamp

Your tool:
  1. Receives: subreddit="MachineLearning", min_score=100, max=20
  2. Fetches hot.json with httpx
  3. Filters posts where score >= min_score
  4. Returns structured list of relevant posts

Rate limits: None for this public endpoint (be reasonable â€” 1 call/min is fine)
```

### HuggingFace API â€” How it Works

```
HOW HUGGINGFACE PUBLIC REST API WORKS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
HuggingFace exposes a public REST API at huggingface.co/api/
No authentication needed for public models and spaces.

Key endpoints:
  Trending models: GET /api/models?sort=downloads&limit=20
  Trending spaces: GET /api/spaces?sort=likes&limit=10
  Daily papers:    GET /api/daily_papers?date=2026-02-23

Each endpoint returns JSON arrays with structured data.
Your tools make direct HTTP GET calls using httpx.
```

### GitHub MCP (Cloud) â€” How it Works

```
HOW GITHUB MCP SERVER WORKS:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
The official GitHub MCP server is hosted at:
  https://api.githubcopilot.com/mcp/

Authentication:
  You create a Personal Access Token (PAT) at:
  github.com/settings/tokens â†’ Fine-grained tokens â†’ New token
  Permission needed: "Contents" read (for releases)
                     "Metadata" read (for repos)

The token goes in your GitHub Actions secrets as GITHUB_TOKEN.
When GitHub Actions calls the GitHub MCP server, it passes this
token in the Authorization header.

Tools available:
  search_repositories  â†’ find trending repos by topic
  list_releases        â†’ get all releases for a repo
  get_release          â†’ get details of a specific release
  read_file            â†’ read any file from any public repo
  search_code          â†’ search code across GitHub

Your pipeline calls these tools for:
  â†’ github.search_repositories(topic="llm", sort="stars", days=1)
  â†’ github.list_releases(repo="langchain-ai/langgraph", since=yesterday)
  â†’ Repeat for all 22 monitored framework repos
```

---

## 16. Detailed Sample Outputs

### What the Pipeline Produces Each Morning

```
DAILY RUN SUMMARY â€” February 23, 2026
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Run Start:    7:00:03 AM IST
Run End:      7:03:47 AM IST
Duration:     3 minutes 44 seconds

DATA COLLECTION:
  Research Server (Horizon):   302 items  âœ… Success
    â†’ ArXiv cs.AI:              48 papers
    â†’ ArXiv cs.LG:              52 papers
    â†’ ArXiv stat.ML:            31 papers
    â†’ Papers With Code:         20 papers
    â†’ Semantic Scholar:         17 papers
    â†’ OpenReview:               14 papers
    â†’ RSS feeds (35 sources): 110 items
    â†’ Kaggle:                    10 items

  Community Server (Horizon):  138 items  âœ… Success
    â†’ r/MachineLearning:         18 posts
    â†’ r/LocalLLaMA:              22 posts
    â†’ r/DeepLearning:            14 posts
    â†’ r/DataScience:             12 posts
    â†’ r/ArtificialIntelligence:  16 posts
    â†’ Hacker News:               21 stories
    â†’ HuggingFace models:        20 items
    â†’ HuggingFace papers:         8 items
    â†’ HuggingFace spaces:         7 items

  GitHub MCP (Cloud):           43 items  âœ… Success
    â†’ Trending repos:            38 repos
    â†’ Framework releases:         5 releases
      (LangGraph v0.2.1, FastAPI 0.111.0,
       CrewAI v0.28.0, Haystack 2.5.0, MLflow 2.15.0)

  TOTAL RAW:                   483 items

DEDUPLICATION:
  After URL dedup:             361 items  (122 removed)
  After fuzzy title dedup:     238 items  (123 removed)
  Total removed:               245 duplicates (51%)

GEMINI PROCESSING:
  Items sent:                  238
  Tokens used:                 ~47,200 (4.7% of daily limit)
  API calls used:              1 (of 1,500 free)
  Breaking items flagged:      4
  Score 9-10:                  31 items
  Score 7-8:                   67 items
  Score 5-6:                   89 items
  Score 1-4:                   51 items

STORAGE:
  news_items saved:            148
  papers saved:                 53
  github_repos saved:           37
  Supabase rows added:         238

PUBLISHING:
  ğŸ“§ Email:     âœ… Sent to your@email.com
  ğŸ¤– Telegram:  âœ… 5 messages sent to chat ID
  ğŸ® Discord:   âœ… 4 webhook calls (38 embeds)
  ğŸ–¥ï¸ Dashboard: âœ… Live on Vercel (auto-updated)

STATUS: SUCCESS âœ…
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

### Sample Telegram Output

```
MESSAGE 1 OF 5 â€” Breaking News
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸš¨ BREAKING AI NEWS â€” Feb 23, 2026

ğŸ”´ [LangGraph v0.2.1 Released](https://github.com/...)
   Score: 10/10
   LangGraph 0.2.1 introduces native streaming API for
   real-time agent state updates and a checkpoint system
   enabling resumable multi-step workflows.

ğŸ”´ [Google Releases Gemini 2.5 Ultra](https://blog.google/...)
   Score: 10/10
   Gemini 2.5 Ultra achieves state-of-the-art on 12 of
   15 benchmarks with a 2M token context window and
   native tool use improvements.


MESSAGE 2 OF 5 â€” Framework Updates
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ§  FRAMEWORK UPDATES TODAY

ğŸ¤– [LangGraph v0.2.1](https://github.com/...) â˜… 10/10
   Streaming API + checkpoint system for resumable agents.
   Tags: Agents Â· LLM Â· OpenSource

ğŸ¤– [CrewAI v0.28.0](https://github.com/...) â˜… 9/10
   New memory module with cross-crew shared context.
   Tags: Agents Â· MultiAgent

âš™ï¸ [FastAPI 0.111.0](https://github.com/...) â˜… 9/10
   WebSocket improvements + new dependency injection patterns.
   Tags: Backend Â· OpenSource

âš™ï¸ [MLflow 2.15.0](https://github.com/...) â˜… 8/10
   LLM evaluation framework expanded with new metrics.
   Tags: MLOps Â· Evaluation


MESSAGE 3 OF 5 â€” Top AI/ML News
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“° TOP AI/ML NEWS TODAY

â€¢ [Anthropic publishes interpretability results](https://...)
  Score: 9/10 | Tags: Research Â· Safety
  Anthropic's sparse autoencoder work reveals internal
  representations for emotion and planning in Claude.

â€¢ [OpenAI open-sources reasoning trace dataset](https://...)
  Score: 8/10 | Tags: LLM Â· Dataset Â· OpenSource
  500k reasoning traces released for fine-tuning
  chain-of-thought reasoning in smaller models.

â€¢ [DeepMind AlphaFold 3 used in drug discovery](https://...)
  Score: 8/10 | Tags: Research Â· Biology Â· LLM
  First peer-reviewed drug candidate designed using
  AlphaFold 3 protein structure predictions.

[+ 7 more stories in dashboard]
ğŸ”— https://your-dashboard.vercel.app


MESSAGE 4 OF 5 â€” GitHub Trending
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ’» TRENDING GITHUB REPOS

â­ [microsoft/phi-4-mini](https://github.com/...)
   New 4B parameter model optimized for edge deployment.
   Python | â­ 2.4k today | Tags: llm, inference

â­ [unslothai/unsloth](https://github.com/...)
   Fine-tuning now 5x faster with 70% less VRAM.
   Python | â­ 1.1k today | Tags: fine-tuning, llm

â­ [modal-labs/quillman](https://github.com/...)
   Real-time voice AI pipeline using Modal + Whisper.
   Python | â­ 890 today | Tags: voice, agents


MESSAGE 5 OF 5 â€” Research Papers
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“„ TOP RESEARCH PAPERS

ğŸ”¬ [Scaling Laws for Agent Tasks](https://arxiv.org/...)
   ArXiv cs.AI | Score: 9/10
   Establishes empirical scaling laws for multi-step
   agentic tasks, showing emergent tool use at 13B params.

ğŸ”¬ [MegaScale-Infer: Serving 1T Parameters](https://arxiv.org/...)
   ArXiv cs.LG | Score: 9/10
   Distributed inference system for trillion-parameter
   models using tensor parallelism on commodity hardware.

ğŸ”¬ [RLHF Without Human Feedback](https://arxiv.org/...)
   NeurIPS 2025 | Score: 8/10
   Self-play method generates synthetic preference data
   matching human RLHF quality at 100x lower cost.
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```

### Sample Discord Embed (Visual Description)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â–ˆâ–ˆâ–ˆâ–ˆ GREEN LEFT BORDER (AgentFramework)                   â”‚
â”‚                                                          â”‚
â”‚  ğŸ¤– LangGraph v0.2.1 Released                           â”‚
â”‚     â†‘ (clickable title â†’ GitHub releases page)          â”‚
â”‚                                                          â”‚
â”‚  LangGraph 0.2.1 introduces a native streaming API      â”‚
â”‚  for real-time agent state updates and a checkpoint      â”‚
â”‚  system enabling resumable multi-step workflows.         â”‚
â”‚                                                          â”‚
â”‚  Category: AgentFramework â”‚ Score: 10/10 â”‚ ğŸ”´ BREAKING  â”‚
â”‚  Tags: Agents Â· LLM Â· OpenSource                        â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  langchain_ai/langgraph  â€¢  Feb 23, 2026  â€¢  GitHub     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

COLOR CODE:
  ğŸŸ¢ Green   = AgentFramework items
  ğŸ”µ Blue    = BackendFramework items
  ğŸŸ  Orange  = Research papers
  ğŸ”´ Red     = Breaking news / ModelRelease
  ğŸŸ£ Purple  = Tools
  ğŸŸ¡ Yellow  = Newsletter items
  ğŸ©µ Cyan    = Community posts (Reddit/HN)
```

### Sample Email Structure

```
EMAIL SUBJECT:
  ğŸ§  AI/ML Digest â€” Feb 23, 2026 | 4 Breaking Â· 5 Framework Updates

EMAIL BODY SECTIONS:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  ğŸ§  AI/ML Daily Digest                Feb 23, 2026     â”‚
  â”‚  238 items processed Â· 4 breaking Â· Gemini Flash 2.0  â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  ğŸš¨ BREAKING TODAY                                     â”‚
  â”‚                                                        â”‚
  â”‚  [LangGraph v0.2.1]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 10/10               â”‚
  â”‚  Summary text...                          [Read â†’]     â”‚
  â”‚                                                        â”‚
  â”‚  [Gemini 2.5 Ultra]  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 10/10               â”‚
  â”‚  Summary text...                          [Read â†’]     â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  ğŸ§  AGENT FRAMEWORKS  â”‚  âš™ï¸ BACKEND FRAMEWORKS          â”‚
  â”‚                        â”‚                               â”‚
  â”‚  LangGraph v0.2.1  10  â”‚  FastAPI 0.111.0  9          â”‚
  â”‚  CrewAI v0.28.0     9  â”‚  MLflow 2.15.0    8          â”‚
  â”‚                        â”‚                               â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  ğŸ“° TOP STORIES                                        â”‚
  â”‚                                                        â”‚
  â”‚  1. Anthropic publishes interpretability results   9/10â”‚
  â”‚     Summary Â· Tags: Research Â· Safety     [Read â†’]     â”‚
  â”‚                                                        â”‚
  â”‚  2. OpenAI open-sources reasoning dataset          8/10â”‚
  â”‚     Summary Â· Tags: LLM Â· Dataset        [Read â†’]     â”‚
  â”‚  [... 6 more stories ...]                              â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  ğŸ’» TRENDING REPOS        ğŸ“„ TOP PAPERS                â”‚
  â”‚  [3 cards]                [3 cards]                    â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  View full digest â†’ your-dashboard.vercel.app          â”‚
  â”‚  Powered by Gemini Flash 2.0 + 40+ MCP sources         â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 17. Step-by-Step Build Timeline â€” Scratch to Production

### Week 1 â€” Accounts, Local Setup, First Working Tool

```
DAY 1: Create All Required Accounts
  â–¡ Google AI Studio (aistudio.google.com) â†’ get Gemini API key
  â–¡ Supabase (supabase.com) â†’ create project, note URL + keys
  â–¡ Prefect Horizon (horizon.prefect.io) â†’ sign in with GitHub
  â–¡ Telegram â†’ create bot via @BotFather â†’ save token
  â–¡ Discord â†’ create webhook in your server â†’ save URL
  â–¡ Resend (resend.com) â†’ create account â†’ get API key
  â–¡ Kaggle (kaggle.com) â†’ create account â†’ download API token
  â–¡ Semantic Scholar (semanticscholar.org/product/api) â†’ free key
  â–¡ OpenReview (openreview.net) â†’ create free account
  â–¡ GitHub â†’ create Personal Access Token (PAT) for GitHub MCP

DAY 2-3: Install Local Development Tools
  â–¡ Install Python 3.11 (python.org)
  â–¡ Install pip: python -m ensurepip
  â–¡ Install FastMCP: pip install fastmcp
  â–¡ Install feedparser: pip install feedparser
  â–¡ Install arxiv: pip install arxiv
  â–¡ Install httpx: pip install httpx
  â–¡ Install crawl4ai: pip install crawl4ai + playwright install chromium
  â–¡ Install kaggle: pip install kaggle
  â–¡ Verify all installs with: pip list
  â–¡ Create GitHub account if not exists
  â–¡ Install Git on your PC (git-scm.com)

DAY 4-5: Build First Tool (RSS) and Test
  â–¡ Create folder: ai-digest-research-server
  â–¡ Create main.py with FastMCP server instance
  â–¡ Write rss/fetch_feed tool (single feed URL)
  â–¡ Add @mcp.tool decorator and docstring
  â–¡ Run: fastmcp dev main.py
  â–¡ Open MCP Inspector in browser
  â–¡ Test rss/fetch_feed with OpenAI blog URL
  â–¡ Verify you get back article titles and summaries
  â–¡ Create config/rss_sources.yaml with first 5 feeds
  â–¡ Write rss/fetch_all tool
  â–¡ Test: all 5 feeds return items in MCP Inspector

DAY 6-7: Complete Research Server Tools
  â–¡ Add arxiv/get_recent tool â†’ test with cs.AI
  â–¡ Add papers/trending tool â†’ test with PWC API
  â–¡ Add scholar/search tool â†’ test with "LLM agents"
  â–¡ Add openreview/neurips tool â†’ test with 2024 venue
  â–¡ Add fetch/url tool â†’ test with HF Forums URL
  â–¡ Add kaggle/datasets tool â†’ test with "llm" tag
  â–¡ Test all tools in MCP Inspector
  â–¡ Create requirements.txt with all dependencies
  â–¡ Create .env.example template

END OF WEEK 1: Research server works locally with 8+ tools tested
```

### Week 2 â€” Community Server + Horizon Deployment

```
DAY 1-2: Build Community Server
  â–¡ Create folder: ai-digest-community-server
  â–¡ Create main.py with FastMCP server instance
  â–¡ Write reddit/r_machinelearning tool
  â–¡ Test: hot.json returns posts correctly
  â–¡ Write reddit/r_localllama, r_deeplearning, r_datascience, r_ai
  â–¡ Write hn/top_ai tool using Algolia API
  â–¡ Test: HN returns AI stories with score filter
  â–¡ Write hf/trending_models tool
  â–¡ Write hf/daily_papers tool
  â–¡ Write hf/trending_spaces tool
  â–¡ Test all tools in MCP Inspector
  â–¡ Create requirements.txt (just httpx + fastmcp)

DAY 3-4: Deploy Research Server to Horizon
  â–¡ Create GitHub repo: ai-digest-research-server
  â–¡ Push main.py, requirements.txt, config/ folder
  â–¡ Go to horizon.prefect.io
  â–¡ Click Deploy â†’ select your repo
  â–¡ Set entrypoint: main.py:mcp
  â–¡ Add environment variables: KAGGLE_USERNAME, KAGGLE_KEY
  â–¡ Click Deploy â†’ wait 60 seconds
  â–¡ Open ChatMCP interface in Horizon
  â–¡ Test EVERY tool in the live Horizon deployment
  â–¡ Copy your live URL and save it

DAY 5-6: Deploy Community Server to Horizon
  â–¡ Create GitHub repo: ai-digest-community-server
  â–¡ Push code to GitHub
  â–¡ Deploy to Horizon (same steps as above)
  â–¡ Test all community tools via ChatMCP
  â–¡ Copy live URL

DAY 7: Build and Deploy Utility Server
  â–¡ Create ai-digest-utility-server folder
  â–¡ Write crawl/page tool using crawl4ai
  â–¡ Write search/web tool (pick a public SearXNG instance)
  â–¡ Write memory tools
  â–¡ Test locally
  â–¡ Deploy to Horizon
  â–¡ Test via ChatMCP

END OF WEEK 2: All 3 custom servers live on Horizon, tested, URLs saved
```

### Week 3 â€” Pipeline Orchestrator + Gemini + Supabase

```
DAY 1-2: Create Supabase Database
  â–¡ Go to Supabase project â†’ SQL Editor
  â–¡ Run schema.sql to create all 4 tables
  â–¡ Verify tables appear in Table Editor
  â–¡ Note: project_ref for Supabase MCP URL
  â–¡ Test Supabase MCP endpoint in browser

DAY 3-4: Build Pipeline Orchestrator
  â–¡ Create folder: ai-digest-pipeline
  â–¡ Create mcp_config.py with all 7 server URLs
  â–¡ Create orchestrator.py that:
     â†’ Calls all 3 Horizon servers in parallel
     â†’ Calls GitHub MCP cloud server
     â†’ Merges all results
  â–¡ Test: run orchestrator.py locally
  â–¡ Verify all servers respond with data
  â–¡ Verify you get ~400+ raw items

DAY 5-6: Add Deduplication and Gemini
  â–¡ Create deduplicator.py
  â–¡ Test deduplication on the 400 raw items
  â–¡ Verify duplicates are removed (should drop to ~200)
  â–¡ Create gemini_client.py
  â–¡ Build the Gemini prompt (JSON instructions)
  â–¡ Test: send 10 items to Gemini, check quality
  â–¡ Test: send all 200 items in one batch
  â–¡ Verify scores, summaries, tags look correct

DAY 7: Add Supabase Saving
  â–¡ Create supabase_client.py
  â–¡ Add save functions for each table
  â–¡ Test: run full pipeline, check Supabase Table Editor
  â–¡ Verify data appears in all 3 tables
  â–¡ Verify digest_runs table has an entry

END OF WEEK 3: Full pipeline runs locally end-to-end
```

### Week 4 â€” Publishers + Automation + Dashboard

```
DAY 1-2: Build Publishers
  â–¡ Create telegram_publisher.py
  â–¡ Test: send sample message to your Telegram bot
  â–¡ Implement all 5 message sections
  â–¡ Add command handlers: /today, /agents, /backend, /papers
  â–¡ Create discord_publisher.py
  â–¡ Test: send embeds to your Discord channel
  â–¡ Verify color coding works
  â–¡ Create email_publisher.py
  â–¡ Test: send sample email via Resend
  â–¡ Check rendering on Gmail + mobile

DAY 3-4: GitHub Actions Setup
  â–¡ Create .github/workflows/daily_digest.yml in pipeline repo
  â–¡ Push to GitHub
  â–¡ Go to repo â†’ Actions â†’ Run Workflow (manual trigger)
  â–¡ Watch the live logs in GitHub Actions
  â–¡ Verify all steps complete successfully
  â–¡ Check Telegram/Discord/Email received the digest
  â–¡ Check Supabase was updated

DAY 5-6: Build Next.js Dashboard
  â–¡ Create Next.js 14 app in frontend/ folder
  â–¡ Install Supabase JS client
  â–¡ Build main feed page (reads news_items table)
  â–¡ Build agents page (filter AgentFramework)
  â–¡ Build backend page (filter BackendFramework)
  â–¡ Build papers page
  â–¡ Build GitHub trending page
  â–¡ Deploy to Vercel (connect GitHub repo)
  â–¡ Add SUPABASE env vars in Vercel settings
  â–¡ Test live dashboard URL

DAY 7: Final Testing + Monitoring Setup
  â–¡ Wait for scheduled 7 AM run (or trigger manually)
  â–¡ Verify all channels received the digest
  â–¡ Check Supabase row count increased
  â–¡ Enable GitHub Actions email notifications for failures
  â–¡ Test each Telegram bot command
  â–¡ Make any formatting adjustments
  â–¡ System is LIVE âœ…

END OF WEEK 4: Fully automated, 24/7, PC-off system
```

---

## 18. Complete Accounts & Keys Setup

Everything you need to sign up for before you start building:

```
REQUIRED â€” Cannot build without these:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1. GitHub Account
   URL:      github.com
   Sign up:  Free
   What for: Store 4 repos + GitHub Actions cron + GitHub MCP
   Key type: Personal Access Token (Fine-grained)
   Where:    github.com/settings/tokens â†’ Fine-grained â†’ New token
   Permissions needed: Contents read, Metadata read
   Free limits: 5,000 API req/hr, 2,000 Actions min/month

2. Prefect Horizon Account
   URL:      horizon.prefect.io
   Sign up:  Sign in with GitHub (no separate registration)
   What for: Free hosting for your 3 FastMCP servers
   Key type: No key needed â€” uses GitHub OAuth
   Free limits: Personal projects free forever

3. Supabase Account
   URL:      supabase.com
   Sign up:  Free (email or GitHub login)
   What for: PostgreSQL database for all digest data
   Key type: Project URL + anon key + service role key
   Where:    Project Settings â†’ API â†’ Project URL and Keys
   Free limits: 500MB, 50,000 rows, 2GB bandwidth/month

4. Google AI Studio (Gemini API)
   URL:      aistudio.google.com
   Sign up:  Sign in with Google account
   What for: Gemini Flash 2.0 for summarization
   Key type: API Key (starts with "AIzaSy...")
   Where:    aistudio.google.com â†’ Get API Key â†’ Create API Key
   Free limits: 1,500 requests/day, 1M tokens/min

5. Telegram Bot
   Process:  Open Telegram â†’ search @BotFather â†’ /newbot
   What you get: BOT_TOKEN (format: 1234:ABCdef...)
   Chat ID:  After creating bot, send /start to it
             Then fetch: api.telegram.org/bot<TOKEN>/getUpdates
             Look for "chat": {"id": YOUR_CHAT_ID}
   Free limits: Unlimited for personal bots

6. Discord Webhook
   Process:  Discord server â†’ any channel â†’ Edit Channel
             â†’ Integrations â†’ Webhooks â†’ New Webhook
   What you get: Webhook URL (long URL starting with discord.com/api/...)
   Free limits: 30 webhook calls/minute per channel

7. Resend Email
   URL:      resend.com
   Sign up:  Free
   What for: Send your daily email digest
   Key type: API Key (starts with "re_...")
   Where:    resend.com â†’ API Keys â†’ Create API Key
   Domain:   Add your domain (for custom from address)
             OR use onboarding@resend.dev for testing
   Free limits: 3,000 emails/month

OPTIONAL â€” For specific data sources:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
8. Kaggle Account
   URL:      kaggle.com
   Sign up:  Free
   What for: kaggle/* tools for datasets and competitions
   Key type: Username + API Key (in kaggle.json file)
   Where:    kaggle.com â†’ Account â†’ Settings â†’ Create New API Token
             Downloads kaggle.json file â€” save username and key from it
   Free limits: Unlimited API access

9. Semantic Scholar (Optional)
   URL:      semanticscholar.org/product/api
   Sign up:  Free
   What for: Higher rate limits for scholar/* tools
             (Works without key at 100 req/5min; with key at 1 req/sec)
   Key type: API Key
   Free limits: 1 request/second with free key

10. OpenReview Account
    URL:     openreview.net
    Sign up: Free (email + password)
    What for: openreview/* tools for conference papers
    Key type: Email + password (passed as env vars)
    Free limits: No stated limits for read access

DASHBOARD ONLY:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
11. Vercel Account
    URL:     vercel.com
    Sign up: Free (sign in with GitHub)
    What for: Host your Next.js dashboard
    Free limits: Unlimited personal projects, 100GB bandwidth/month
```

---

## 19. Final Cost Summary

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘              COMPLETE MONTHLY COST BREAKDOWN                  â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                              â•‘
â•‘  HOSTING                                                     â•‘
â•‘  Prefect Horizon (3 FastMCP servers)  FREE for personal      â•‘
â•‘  Vercel (Next.js dashboard)           FREE unlimited         â•‘
â•‘  GitHub Actions (cron scheduler)      FREE 2000 min/month    â•‘
â•‘                                                              â•‘
â•‘  DATABASES & APIS                                            â•‘
â•‘  Supabase (PostgreSQL DB)             FREE 500MB             â•‘
â•‘  Gemini Flash 2.0 (AI summaries)      FREE 1,500 req/day     â•‘
â•‘                                                              â•‘
â•‘  NOTIFICATIONS                                               â•‘
â•‘  Telegram Bot API                     FREE unlimited         â•‘
â•‘  Discord Webhooks                     FREE unlimited         â•‘
â•‘  Resend (email)                       FREE 3,000 emails/mo   â•‘
â•‘                                                              â•‘
â•‘  DATA SOURCES (all MCP servers)                              â•‘
â•‘  GitHub MCP (official cloud)          FREE with GitHub acct  â•‘
â•‘  HuggingFace MCP (official cloud)     FREE with HF account   â•‘
â•‘  Supabase MCP (official cloud)        FREE your own DB       â•‘
â•‘  Context7 MCP (context7.com)          FREE personal tier     â•‘
â•‘  ArXiv API (no MCP needed)            FREE always            â•‘
â•‘  Reddit JSON API                      FREE always            â•‘
â•‘  HN Algolia API                       FREE always            â•‘
â•‘  OpenReview API                       FREE always            â•‘
â•‘  Papers With Code API                 FREE always            â•‘
â•‘  Kaggle API                           FREE with account      â•‘
â•‘  Semantic Scholar API                 FREE with free key     â•‘
â•‘  RSS/Atom feeds (35+ sources)         FREE always            â•‘
â•‘                                                              â•‘
â•‘  YOUR USAGE vs FREE LIMITS                                   â•‘
â•‘  Gemini:         1-2 calls/day vs 1,500 limit  = 0.1%       â•‘
â•‘  Supabase:       ~300 rows/day vs 50k limit    = growing     â•‘
â•‘                  Enough for 4+ months of data               â•‘
â•‘  GitHub Actions: 4 min/day vs 2,000 min/month  = 6%         â•‘
â•‘  Resend:         1 email/day vs 3,000/month    = 1%          â•‘
â•‘                                                              â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  TOTAL MONTHLY COST:                  $0.00                  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## 20. Final Launch Checklist

### Accounts Verified
```
â–¡ GitHub: Account exists, PAT created with correct permissions
â–¡ Horizon: Account created, signed in with GitHub
â–¡ Supabase: Project created, schema SQL executed, tables visible
â–¡ Google AI Studio: API key created and tested (returns summaries)
â–¡ Telegram: Bot created, token saved, your chat ID found and verified
â–¡ Discord: Webhook URL created for target channel
â–¡ Resend: Account created, sender verified, API key saved
â–¡ Kaggle: Account created, kaggle.json downloaded, credentials saved
â–¡ Semantic Scholar: Free API key created (optional but recommended)
â–¡ OpenReview: Free account created, email and password saved
â–¡ Vercel: Account created, signed in with GitHub
```

### Server 1 (Research) â€” Verified
```
â–¡ All tools defined and decorated with @mcp.tool
â–¡ All docstrings written (describes what tool does, what it returns)
â–¡ rss_sources.yaml contains all 35+ feed URLs
â–¡ Tested every tool in FastMCP Inspector locally â€” all return data
â–¡ requirements.txt complete
â–¡ Pushed to GitHub repo: ai-digest-research-server
â–¡ Deployed to Horizon â€” status shows green/running
â–¡ All tools tested in Horizon ChatMCP interface with real data
â–¡ Horizon URL saved: https://ai-digest-research.YOUR_NAME.fastmcp.app/mcp
```

### Server 2 (Community) â€” Verified
```
â–¡ All Reddit tools return posts with correct score filtering
â–¡ HN tool returns AI stories with 50+ score
â–¡ HF tools return models/spaces/papers
â–¡ Tested in FastMCP Inspector locally
â–¡ Pushed to GitHub repo: ai-digest-community-server
â–¡ Deployed to Horizon â€” status green
â–¡ All tools tested in Horizon ChatMCP
â–¡ Horizon URL saved
```

### Server 3 (Utility) â€” Verified
```
â–¡ crawl/page tool tested on a JS-heavy page (Medium article)
â–¡ search/web tool returns results from SearXNG
â–¡ memory tools save and recall correctly
â–¡ Deployed to Horizon â€” status green
â–¡ Horizon URL saved
```

### Cloud MCPs â€” Configured
```
â–¡ GitHub MCP URL: api.githubcopilot.com/mcp/ â€” tested with PAT
â–¡ Supabase MCP URL: mcp.supabase.com â€” tested with project ref
â–¡ Context7 â€” tested (npx @upstash/context7-mcp)
â–¡ All URLs saved in mcp_config.py
```

### Database â€” Verified
```
â–¡ All 4 Supabase tables created: news_items, papers, github_repos, digest_runs
â–¡ All indexes created
â–¡ Test insert and query work correctly
â–¡ Supabase MCP can query your DB
```

### Pipeline â€” Verified
```
â–¡ orchestrator.py calls all 7 MCP servers successfully
â–¡ Returns 400+ raw items from a real test run
â–¡ Deduplication reduces to ~200 unique items
â–¡ Gemini returns summaries and scores for all items
â–¡ Quality of summaries looks good (technical, accurate)
â–¡ Saving to Supabase works â€” rows appear in table editor
â–¡ digest_runs table shows a success entry
```

### Publishers â€” Verified
```
â–¡ Telegram: Received full formatted digest in your chat
â–¡ Telegram: /today command returns today's top 10 items
â–¡ Telegram: /agents returns only AgentFramework items
â–¡ Telegram: /backend returns only BackendFramework items
â–¡ Discord: Embeds appear with correct color coding
â–¡ Discord: Breaking news appears as red embed at top
â–¡ Email: Received in inbox (not spam folder)
â–¡ Email: Renders correctly on mobile and desktop
```

### GitHub Actions â€” Verified
```
â–¡ .github/workflows/daily_digest.yml committed to pipeline repo
â–¡ All 13 secrets added to repo Settings â†’ Secrets
â–¡ Manual trigger (Run Workflow) completed successfully
â–¡ All steps show green checkmarks in Actions log
â–¡ Failure notification email enabled in Actions settings
â–¡ Cron schedule confirmed: 30 1 * * * = 7:00 AM IST
```

### Dashboard â€” Verified
```
â–¡ Next.js app deployed on Vercel
â–¡ Supabase env vars added in Vercel project settings
â–¡ Main feed loads with today's items sorted by score
â–¡ Breaking banner appears correctly
â–¡ /agents page filters correctly
â–¡ /backend page filters correctly
â–¡ /papers page shows abstracts
â–¡ /github page shows star counts
â–¡ /archive date picker works
â–¡ Search bar returns results
```

### Final Live Test
```
â–¡ Waited for scheduled 7:00 AM run (or triggered manually)
â–¡ All 4 channels received the digest simultaneously
â–¡ Supabase row count increased by expected amount
â–¡ digest_runs table shows status: "success"
â–¡ PC was off during the run â€” everything still worked
â–¡ SYSTEM IS FULLY LIVE âœ…
```

---

## Appendix A â€” Where to Get Everything

```
FRAMEWORKS & LIBRARIES:
FastMCP Documentation:      gofastmcp.com
FastMCP GitHub:             github.com/PrefectHQ/fastmcp
FastMCP PyPI:               pip install fastmcp
FastMCP Discord:            discord.gg/fastmcp

MCP Official Documentation: modelcontextprotocol.io
MCP Python SDK:             github.com/modelcontextprotocol/python-sdk

DATA SOURCE LIBRARIES:
feedparser (RSS):           feedparser.readthedocs.io
                            pip install feedparser
arxiv Python library:       lukasschwab.me/arxiv.py
                            pip install arxiv
httpx (HTTP client):        www.python-httpx.org
                            pip install httpx
crawl4ai:                   crawl4ai.com
                            pip install crawl4ai
kaggle:                     github.com/Kaggle/kaggle-api
                            pip install kaggle

CLOUD SERVICES:
Prefect Horizon:            horizon.prefect.io
GitHub (Actions + MCP):     github.com / api.githubcopilot.com/mcp/
HuggingFace MCP:            huggingface.co/mcp
Supabase:                   supabase.com / mcp.supabase.com
Context7:                   context7.com / upstash/context7-mcp
SearXNG public instances:   searx.space
Google AI Studio:           aistudio.google.com
Vercel:                     vercel.com
Resend:                     resend.com

APIs (all free):
ArXiv API:                  info.arxiv.org/help/api/index.html
Papers With Code API:       paperswithcode.com/api/v1/docs
Semantic Scholar API:       api.semanticscholar.org/api-docs
OpenReview API:             docs.openreview.net/reference/api-v2
HuggingFace API:            huggingface.co/docs/api-inference
Reddit JSON:                reddit.com/r/SUBREDDIT/hot.json
HN Algolia API:             hn.algolia.com/api
Kaggle API:                 kaggle.com/docs/api
Stack Exchange API:         api.stackexchange.com
```

---

*Built to run forever at $0/month Â· PC never required Â· Fully automated Â· 95%+ source coverage*
*FastMCP + Prefect Horizon + GitHub Actions + Gemini Flash 2.0 + Supabase + Vercel*
